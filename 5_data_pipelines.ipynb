{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_data_pipelines.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J4mwmBwdyCv4"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO+aFhextj7BFJBsU/4Gf7N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendenwest/ad450/blob/master/5_data_pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipelines\n",
        "\n",
        "### Reading\n",
        "- https://spark.apache.org/docs/latest/api/python/getting_started/index.html\n",
        "- https://sparkbyexamples.com/pyspark-tutorial/#rdd"
      ],
      "metadata": {
        "id": "_bvfnGWRsi3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# What is PySpark?\n",
        "\n",
        "Apache Spark is an open-source enginer for performing large-scale, distributed (parallelized) data-processing & machine learning operations.\n",
        "\n",
        "PySpark is a Spark library written in Python and compatible with Pandas. PySpark can run operations 100x faster than a traditional python application.\n",
        "\n",
        "Some key advantages of PySpark:\n",
        "- distributed (cluster) processing\n",
        "- in-memory computation\n",
        "- immutable data objects\n",
        "- lazy evaluation\n",
        "- caching & persistence\n",
        "- ANSI SQL compatibility\n"
      ],
      "metadata": {
        "id": "wpvU9pK1t7n7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Objects\n",
        "\n",
        "**RDD (Resilient Distributed Dataset)**\n",
        "\n",
        "A fundamental data structure of PySpark. An RDD is a fault-tolerant, immutable distributed collection of objects. \n",
        "\n",
        "Each dataset in an RDD is divided into logical partitions, which can be computed on different nodes of a cluster.\n",
        "\n",
        "**SparkSession** An entrypoint to the PySpark application\n",
        "\n",
        "**SparkContext** \n",
        "\n",
        "**PySpark DataFrame**\n",
        "\n",
        "A distributed collection of data organized into named columns. Conceptually equivalent to relational database table or a Pandas dataframe, but with richer optimizations under the hood. PySpark executes DataFrames in parallel across clustered machines.\n",
        "\n",
        "DataFrames can be constructed from a wide array of sources such as structured data files, tables in Hive, external databases, or existing RDDs."
      ],
      "metadata": {
        "id": "E5WyU00yydlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDD Operations\n",
        "\n",
        "The PySpark RDD supports two kinds of operations:\n",
        "\n",
        "- **RDD transformations** – operations that return a new RDD. Transformations are lazy (executed only when you call an action on the RDD). Some common transformations are - flatMap(), map(), reduceByKey(), filter(), sortByKey()\n",
        "\n",
        "- **RDD actions** – operations that trigger computation and return RDD values to the driver. Any RDD function that returns non-RDD[T] is considered as an action - for example: count(), collect(), first(), max(), reduce()\n",
        "\n"
      ],
      "metadata": {
        "id": "L6nSQcJw0Ffz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "J4mwmBwdyCv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set environment\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "HRXWPxONogHB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "f8I65LkXomJS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a SparkSession\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "\n",
        "  {\"title\": \"Northanger Abbey\", \"author\": \"Austen, Jane\", \"year_written\": 1814, \"edition\": \"Penguin\", \"price\":  18.2}"
      ],
      "metadata": {
        "id": "l5At4V0_otQC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oZduJ6jQmywQ"
      },
      "outputs": [],
      "source": [
        "# install JVM\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install Spark & Hadoop\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "JjfnGccvnCwL"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}