{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_natural_language_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendenwest/ad450/blob/master/10_natural_language_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhAyzTiSvQ1D"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "### Reading\n",
        "- https://towardsdatascience.com/gentle-start-to-natural-language-processing-using-python-6e46c07addf3\n",
        "- https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63\n",
        "\n",
        "### Tutorials\n",
        "- https://spacy.io/usage/spacy-101\n",
        "- https://realpython.com/natural-language-processing-spacy-python/\n",
        "- https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
        "- https://www.datacamp.com/community/tutorials/machine-learning-hotel-reviews\n",
        "- https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python\n",
        "\n",
        "### Practice\n",
        "- https://learn.datacamp.com/courses/introduction-to-natural-language-processing-in-python\n",
        "\n",
        "## Reference\n",
        "- [SPACy](https://spacy.io/usage/spacy-101)\n",
        "- [NLTK](https://www.nltk.org/)\n",
        "- [regular expressions](https://digitalfortress.tech/tricks/top-15-commonly-used-regex/)\n",
        "\n",
        "\n",
        "### Learning Outcomes\n",
        "- what is natural language processing\n",
        "- common Python libraries\n",
        "- word tokenization\n",
        "- stemming & lemmatization\n",
        "- n-grams\n",
        "- Text classification & sentiment analysis\n",
        "- word vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV-oSeUcoU84"
      },
      "source": [
        "# Overview\n",
        "\n",
        "Natural Language Processing (NLP) applies data analysis and machine learning techniques to understanding text and speech.\n",
        "\n",
        "Some common use cases for NLP are:\n",
        "- word frequency analysis\n",
        "- sentiment analysis\n",
        "- plagiarism detection\n",
        "- text-to-speech conversion\n",
        "- machine translation\n",
        "- text-generation (chat bots)\n",
        "\n",
        "## Common libraries\n",
        "\n",
        "The Python ecosystem has two widely used NLP libraries that provide proven solutions for routine text-processing tasks:\n",
        "\n",
        "- [Natural Language ToolKit (NLTK)](https://www.nltk.org/) - Leading platform for python NLP programming, with interfaces to many lexical resources.\n",
        "- [spaCy](https://spacy.io/) - a powerful, open-source library for advanced LP designed specifically for production use.\n",
        "\n",
        "Either library is a solid choice for NLP programming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUs86KIS3ny9"
      },
      "source": [
        "# Common NLP Techniques\n",
        "\n",
        "- **Tokenization** - Segmenting text into words, punctuations marks etc.\n",
        "- **Part-of-speech (POS) Tagging** - Assigning word types to tokens, like verb or noun. \n",
        "- **Stemming & Lemmatization** - Reduce word variants to a common base form. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.\n",
        "- **Sentence Boundary Detection (SBD)** - Finding and segmenting text into individual sentences. \n",
        "- **Named Entity Recognition (NER)** - Labelling named “real-world” objects, like persons, companies or locations. |\n",
        "- **Entity Linking (EL)** - Disambiguating textual entities to unique identifiers in a knowledge base.\n",
        "- **Similarity** - Comparing words, text spans and documents and how similar they are to each other. \n",
        "- **Text Classification** - Assigning categories or labels to a whole document, or parts of a document.\n",
        "- **Rule-based Matching** - Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
        "\n",
        "\n",
        "- **Stop words** are words which are filtered out before or after processing of text. They usually refer to the most common words in a language.\n",
        "- A **regular expression** is a sequence of characters that define a search pattern.\n",
        "- The bag-of-words model is a simple feature extraction technique that describes the occurrence of each word within a document.\n",
        "- **TF-IDF** is a statistical measure used to evaluate a word's **importance**  to a document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPsedxF7ThrF"
      },
      "source": [
        "## Tokenization\n",
        "Tokenization segments text into words, punctuation and so on, applying rules specific to each language."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, end=\" | \")"
      ],
      "metadata": {
        "id": "Aqw-oGX0DdDM",
        "outputId": "5ca8a108-9bb1-4c18-a623-db361286aa01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple | is | looking | at | buying | U.K. | startup | for | $ | 1 | billion | "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming & Parts of Speech\n",
        "Part-of-speech (POS) Tagging - Assigns word types to tokens, like verb or noun."
      ],
      "metadata": {
        "id": "xAQ6eqN0FigZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{0:8} {1:8} {2:8} {3:8} {4:8} {5:8} {6:4} {7:4}\".format(\"TEXT\",\"LEMMA\",\"POS\",\"TAG\",\"DEP\",\"SHAPE\",\"ALPHA\",\"STOP\"))\n",
        "print(\"--\" * 32)\n",
        "for token in doc:\n",
        "    print(\"{0:8} {1:8} {2:8} {3:8} {4:8} {5:8} {6:4} {7:4} \".format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop))"
      ],
      "metadata": {
        "id": "1UqsLLqGELLE",
        "outputId": "e40e8967-e84d-45e3-8e24-dddd6a682c99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT     LEMMA    POS      TAG      DEP      SHAPE    ALPHA STOP\n",
            "----------------------------------------------------------------\n",
            "Apple    Apple    PROPN    NNP      nsubj    Xxxxx       1    0 \n",
            "is       be       AUX      VBZ      aux      xx          1    1 \n",
            "looking  look     VERB     VBG      ROOT     xxxx        1    0 \n",
            "at       at       ADP      IN       prep     xx          1    1 \n",
            "buying   buy      VERB     VBG      pcomp    xxxx        1    0 \n",
            "U.K.     U.K.     PROPN    NNP      compound X.X.        0    0 \n",
            "startup  startup  NOUN     NN       dobj     xxxx        1    0 \n",
            "for      for      ADP      IN       prep     xxx         1    1 \n",
            "$        $        SYM      $        quantmod $           0    0 \n",
            "1        1        NUM      CD       compound d           0    0 \n",
            "billion  billion  NUM      CD       pobj     xxxx        1    0 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oscXzCrUUjIY"
      },
      "source": [
        "### Stemming & Lemmatization\n",
        "\n",
        "Stemming and lemmatization are special, but different, cases of text **normalization**. \n",
        "\n",
        "- **Stemming** is a crude heuristic process that chops off the ends of words in the hope of achieving a correct *base form*.\n",
        "\n",
        "- **Lemmatization** uses vocabulary and morphological analysis of words to remove grammatical endings only and to return the base or dictionary form of a word (the **lemma**).\n",
        "\n",
        "A stemmer operates without knowledge of the context, and cannot understand the difference between words which have different meaning depending on part of speech. But the stemmers are easier to implement and usually run faster."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition\n",
        "A **named entity** is a “real-world object” with a name – for example, a person, a country, a product or a book title. Entity recognition may use a **corpus** of known entities or a statistical model."
      ],
      "metadata": {
        "id": "laW11kChGeDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{0:10} {1:5} {2:5} {3:5}\".format(\"TEXT\",\"START\",\"END\",\"LABEL\",\"DESCRIPTION\"))\n",
        "print(\"--\" * 15)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(\"{0:10} {1:5} {2:5} {3:5} \".format(ent.text, ent.start_char, ent.end_char, ent.label_))"
      ],
      "metadata": {
        "id": "Qlfoli42GUkt",
        "outputId": "05dac0c5-557b-4509-90be-28d24cc9fcf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT       START END   LABEL\n",
            "------------------------------\n",
            "Apple          0     5 ORG   \n",
            "U.K.          27    31 GPE   \n",
            "$1 billion    44    54 MONEY \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebe-sO8WTkHO"
      },
      "source": [
        "## Word Vectors & Similarity\n",
        "\n",
        "Machine learning algorithms cannot work with raw text directly, so we convert the text into vectors of numbers (`word vectors` or `word embeddings`). \n",
        "\n",
        "Word vectors represent each word in a text numerically such that the vector corresponds to how that word is used or what it means. This allows algorithms to determine similarity of text.\n",
        "\n",
        "One common approach is the `bag-of-words` method, which describes the occurrence of every word within a document. The simplest method for scoring words in text is to mark the presence of words with 1 for present and 0 for absence. \n",
        "\n",
        "Other more complex algorithms, such as `word2vec` derive vectors that take account of the word's context. Words that appear in similar contexts will have similar vectors. Relations between words can be examined with mathematical (matrix) operations.\n",
        "\n"
      ]
    }
  ]
}