{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9_tree_based_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXglYBa/MUvV/VgyYK4L0e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendenwest/ad450/blob/master/9_tree_based_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zUi5tuyiUjT",
        "colab_type": "text"
      },
      "source": [
        "# Decision Trees and Tree-based Models\n",
        "\n",
        "### Reading\n",
        "- Geron - chapter 6 ([Decision trees](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch06.html#trees_chapter)) & chapter 7 ([Ensembles](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch07.html#ensembles_chapter))\n",
        "\n",
        "### Tutorials\n",
        "- https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
        "- https://www.datacamp.com/community/tutorials/kaggle-tutorial-machine-learning (Decision Tree Classifiers)\n",
        "- https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
        "\n",
        "\n",
        "### Learning Outcomes\n",
        "- Decision tree models\n",
        "- Measuring entropy & information gain\n",
        "- Using decision trees for regression\n",
        "- Ensemble learning\n",
        "- Random forests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQwOc1pVAMFu",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Decision trees are flowchart-like tree structures, where each node represents a feature (attribute). Branches represent a decision rule and each leaf node represents the decision outcome.\n",
        "\n",
        "\n",
        "**Pros:**\n",
        "- training time is faster than more complex ML algorithms. Time complexity depends on number of records and number of attributes\n",
        "- Easy to interpret & visualize\n",
        "- can easily capture non-linear patterns\n",
        "- can handle high-dimensional data with good accuracy\n",
        "- can be used for feature engineering \n",
        "- does not depend on probability distribution assumptions\n",
        "\n",
        "**Cons:**\n",
        "- sensitive to noisy data & prone to overfit\n",
        "- sensitive to variation in dataset (can be addressed with **bagging** & **boosting**)\n",
        "- biased with unbalanced dataset\n",
        "\n",
        "How the Decision Tree algorithm works:\n",
        "\n",
        "1.   Selecting the best attribute using an Attribute Selection Measure (ASM)\n",
        "2.   Make that attribute a decision node & partition the dataset on it\n",
        "3.   Repeast this process recursively for each child node until one of these conditions match:\n",
        "     - all tuples belong to the same attribute value\n",
        "     - there are no more attributes\n",
        "     - there are no more instances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKBuhtg0Dgmj",
        "colab_type": "text"
      },
      "source": [
        "## Attribute Selection Measures (ASM)\n",
        "\n",
        "Refers to methods for partitioning a dataset in the way that best explains it. \n",
        "\n",
        "Each attribute is ranked by how well it explains the dataset, and the attribute with the best score is selected.\n",
        "\n",
        "For attributes with continuous values, it's necessary to define split points.\n",
        "\n",
        "\n",
        "### Information Gain\n",
        "\n",
        "Information gain is a decrease in **entropy** (randomness or impurity) in a dataset.\n",
        "\n",
        "The Information Gain ASM computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values.\n",
        "\n",
        "The attribute with highest information gain - at a given decision node - is chosen as the splitting attribute.\n",
        "\n",
        "Information Gain is biased for the attribute with a large number of distinct values.\n",
        "\n",
        "### Gain Ratio\n",
        "\n",
        "Gain ratio handles the issue of bias by normalizing the information gain using Split Info.\n",
        "\n",
        "### Gini Index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7epq91FK_Rx",
        "colab_type": "text"
      },
      "source": [
        "## Optimizing Decision Tree Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5skVQEz-LV0r",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing Decision Trees"
      ]
    }
  ]
}