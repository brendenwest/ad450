{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_ml_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8Ut0LKGCThKJeSzdof5HE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendenwest/ad450/blob/master/7_ml_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy-_gSjBPelf",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning with Regression \n",
        "\n",
        "### Reading\n",
        "- Géron - Chapter 2: End-to-End Machine Learning Project\n",
        "- https://www.inferentialthinking.com/chapters/15/Prediction.html\n",
        "- https://www.inferentialthinking.com/chapters/16/Inference_for_Regression.html\n",
        "\n",
        "(optional)\n",
        "- https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9\n",
        "- http://greenteapress.com/thinkstats2/html/thinkstats2011.html\n",
        "- http://greenteapress.com/thinkstats2/html/thinkstats2012.html (through 11.5)\n",
        "\n",
        "### Tutorials\n",
        "- https://www.datacamp.com/community/tutorials/essentials-linear-regression-python\n",
        "- https://www.coursera.org/learn/ml-regression\n",
        "\n",
        "\n",
        "### Learning Outcomes\n",
        "- overview of linear regression\n",
        "- problems suited for linear regression\n",
        "- measuring feature correlations\n",
        "- encoding text & categorical data\n",
        "- univariate & multi-variate regression\n",
        "- measuring performance of regression models\n",
        "- cost functions\n",
        "- optimizing with gradient descent\n",
        "- cross validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSQxXDYXyPDP",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression Overview\n",
        "\n",
        "the principle behind the working of linear regression is very important as to reason the evolution of a whole class of statistical algorithms called **Generalized Linear Models**\n",
        "\n",
        "used for predictive/statistical modeling of datasets with continuous numeric labels (target)\n",
        "\n",
        "used to predict the value of a response (dependent) variable from one or more predictor (independent) variables, where the variables are numeric.\n",
        "\n",
        "defines a function that maps feature values to a numeric target (predictor):\n",
        "\n",
        "> h(x) = $θ_{0}$ + $θ_{1}$$x_{1}$\n",
        "\n",
        "Where $θ_{i}$’s are the function parameters (also called weights). Similar to algebraic equations:\n",
        "\n",
        "> y = f(x)\n",
        "\n",
        "- **Simple Linear Regression** (SLR) deals with just two variables \n",
        "- **Multi-linear Regression** (MLR) deals with more than two variables \n",
        "\n",
        "Regression models are developed to make h(x) close to y, at least for the training examples. \n",
        "\n",
        "- Linear regression assumes a linear relationship between dependent & independent variables. If that's not the case, data can be transformed so that a linear relationship is maintained. \n",
        "- If dataset features are very correlated to each other, linear regression fails to approximate the relationship appropriately and tends to overfit. So, it is efficient to detect the highly correlated features and to drop them before applying linear regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F09VXT8A5tj",
        "colab_type": "text"
      },
      "source": [
        "### Regression Modeling\n",
        "\n",
        "The **Ordinary Least Squares** (OLS) procedure seeks to minimize the sum of the squared residuals - Root Mean Square Error (RMSE). \n",
        "\n",
        "This means that given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together. \n",
        "\n",
        "RMSE performs very well and is generally preferred, but is sensitive to outliers. If outliers are not rare, other measures of error may be more appropriate.\n",
        "\n",
        "**Least Squares Regression** - trains a model by choosing paramter (θ) values to minimize total error. Starts with an initial guess & iteratively changes θ to make J(θ) smaller, until convergence on a value of θ that minimizes J(θ).  \n",
        "\n",
        "**Gradient descent** is a commonly used convergence algorithm that starts with some initial θ, and repeatedly updates it in the direction towards minimizing the error, using a **learning rate** for the size of the improvement step to take on each iteration.\n",
        "\n",
        "- **batch gradient descent** looks at every example in the training set on each step.\n",
        "- **stochastic (incremental) gradient descent** updates the weighting parameters according to the gradient of the error with respect to just the training example encountered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTacC1UZEI2y",
        "colab_type": "text"
      },
      "source": [
        "### Understanding Model Measures\n",
        "\n",
        "- **R-Squared** - the percent of dependent variable's variance explained by the model. Doesn't tell whether a chosen model is good or bad, nor whether the data and predictions are biased.\n",
        "- **coefficient (coef)** - amount the dependent variable changes if the independent variable changes by 1. For model with a single independent variable, this is the slope of the regression line.\n",
        "- **95% confidence intervals** for the coefficient\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcgwDE0VBQH0",
        "colab_type": "text"
      },
      "source": [
        "### Correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK0rXV6OByrF",
        "colab_type": "text"
      },
      "source": [
        "### Handling Text and Categorical Attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QHQz_pOB7Oz",
        "colab_type": "text"
      },
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9IwbDnzCHVM",
        "colab_type": "text"
      },
      "source": [
        "### Cross Validation"
      ]
    }
  ]
}